{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶å DINO Animal Detection - HSE Zapovednik\n",
        "\n",
        "**–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ñ–æ—Ç–æ–ª–æ–≤—É—à–µ–∫ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ DINO**\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç:\n",
        "- –ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ—Ç–µ–∫—Ü–∏—é –∂–∏–≤–æ—Ç–Ω—ã—Ö –Ω–∞ –≤–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "- –ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–¥–∞—Ç–∞, –≤—Ä–µ–º—è, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞) —á–µ—Ä–µ–∑ OCR\n",
        "- –†–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC AUC –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)\n",
        "- –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\n",
        "!nvidia-smi\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# –ö–õ–û–ù–ò–†–û–í–ê–ù–ò–ï –†–ï–ü–û–ó–ò–¢–û–†–ò–Ø\n",
        "# =====================================================\n",
        "# –ó–∞–º–µ–Ω–∏—Ç–µ YOUR_USERNAME –Ω–∞ –≤–∞—à GitHub username\n",
        "\n",
        "!git clone https://github.com/YOUR_USERNAME/hse-zapovednik.git\n",
        "%cd hse-zapovednik\n",
        "print(\"\\n[OK] –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "print(\"[SETUP] –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...\")\n",
        "%pip install -q timm pycocotools easyocr scikit-learn Pillow tqdm\n",
        "print(\"[OK] –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ö–æ–º–ø–∏–ª—è—Ü–∏—è CUDA –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è DINO (Deformable Attention)\n",
        "import os\n",
        "os.chdir('DINO/models/dino/ops')\n",
        "!python setup.py build install\n",
        "os.chdir('../../../..')\n",
        "print(\"\\n[OK] CUDA –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–°–∫–∞—á–∞–π—Ç–µ checkpoint —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ / Google Drive –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤ Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è checkpoint\n",
        "!mkdir -p output_dino\n",
        "\n",
        "# =====================================================\n",
        "# –í–ê–†–ò–ê–ù–¢ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Colab\n",
        "# =====================================================\n",
        "from google.colab import files\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ checkpoint.pth:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª\n",
        "import shutil\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, 'output_dino/checkpoint.pth')\n",
        "    print(f\"[OK] Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: output_dino/checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "\n",
        "**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:**\n",
        "\n",
        "### COCO —Ñ–æ—Ä–º–∞—Ç:\n",
        "```\n",
        "dataset/\n",
        "‚îú‚îÄ‚îÄ annotations/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ instances_val2017.json\n",
        "‚îî‚îÄ‚îÄ val2017/\n",
        "    ‚îî‚îÄ‚îÄ *.jpg, *.png\n",
        "```\n",
        "\n",
        "### YOLO —Ñ–æ—Ä–º–∞—Ç (Ultralytics):\n",
        "```\n",
        "dataset/\n",
        "‚îú‚îÄ‚îÄ labels/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ *.txt\n",
        "‚îî‚îÄ‚îÄ images/\n",
        "    ‚îî‚îÄ‚îÄ *.jpg, *.png\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (ZIP –∞—Ä—Ö–∏–≤)\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∞—Ä—Ö–∏–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"[INFO] –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {filename}...\")\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('dataset')\n",
        "    print(f\"[OK] –î–∞—Ç–∞—Å–µ—Ç —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω –≤ –ø–∞–ø–∫—É 'dataset'\")\n",
        "\n",
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
        "!ls -la dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ò –ê–ù–ê–õ–ò–ó–ê - –ò–ó–ú–ï–ù–ò–¢–ï –ü–û–î –°–í–û–ô –î–ê–¢–ê–°–ï–¢\n",
        "# =====================================================\n",
        "\n",
        "# –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
        "DATASET_PATH = \"dataset\"\n",
        "\n",
        "# –§–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞: \"coco\", \"yolo\", –∏–ª–∏ \"none\" (—Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)\n",
        "FORMAT = \"coco\"\n",
        "\n",
        "# –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ (0.0 - 1.0)\n",
        "THRESHOLD = 0.3\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "USE_OCR = True\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"–ü–ê–†–ê–ú–ï–¢–†–´ –ê–ù–ê–õ–ò–ó–ê\")\n",
        "print(\"=\"*60)\n",
        "print(f\"–î–∞—Ç–∞—Å–µ—Ç:  {DATASET_PATH}\")\n",
        "print(f\"–§–æ—Ä–º–∞—Ç:   {FORMAT}\")\n",
        "print(f\"–ü–æ—Ä–æ–≥:    {THRESHOLD}\")\n",
        "print(f\"OCR:      {'–î–∞' if USE_OCR else '–ù–µ—Ç'}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
        "import os\n",
        "\n",
        "def find_paths(dataset_path, format_type):\n",
        "    images_dir = None\n",
        "    annotations = None\n",
        "    \n",
        "    if format_type == \"coco\":\n",
        "        # –ò—â–µ–º –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
        "        for subdir in ['val2017', 'images/val', 'images', '']:\n",
        "            path = os.path.join(dataset_path, subdir) if subdir else dataset_path\n",
        "            if os.path.exists(path) and any(f.endswith(('.jpg', '.png', '.jpeg')) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))):\n",
        "                images_dir = path\n",
        "                break\n",
        "        \n",
        "        # –ò—â–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
        "        ann_dir = os.path.join(dataset_path, 'annotations')\n",
        "        if os.path.exists(ann_dir):\n",
        "            for ann_file in ['instances_val2017.json', 'instances_val.json']:\n",
        "                ann_path = os.path.join(ann_dir, ann_file)\n",
        "                if os.path.exists(ann_path):\n",
        "                    annotations = ann_path\n",
        "                    break\n",
        "            if not annotations:\n",
        "                json_files = [f for f in os.listdir(ann_dir) if f.endswith('.json')]\n",
        "                if json_files:\n",
        "                    annotations = os.path.join(ann_dir, json_files[0])\n",
        "    \n",
        "    elif format_type == \"yolo\":\n",
        "        for subdir in ['images/val', 'images', '']:\n",
        "            path = os.path.join(dataset_path, subdir) if subdir else dataset_path\n",
        "            if os.path.exists(path):\n",
        "                images_dir = path\n",
        "                break\n",
        "        \n",
        "        for subdir in ['labels/val', 'labels']:\n",
        "            path = os.path.join(dataset_path, subdir)\n",
        "            if os.path.exists(path):\n",
        "                annotations = path\n",
        "                break\n",
        "    else:\n",
        "        images_dir = dataset_path\n",
        "    \n",
        "    return images_dir, annotations\n",
        "\n",
        "images_dir, annotations = find_paths(DATASET_PATH, FORMAT)\n",
        "\n",
        "print(f\"[INFO] –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {images_dir}\")\n",
        "print(f\"[INFO] –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {annotations if annotations else '–Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}\")\n",
        "\n",
        "if images_dir and os.path.exists(images_dir):\n",
        "    img_count = len([f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
        "    print(f\"[INFO] –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {img_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê\n",
        "# =====================================================\n",
        "\n",
        "# –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É\n",
        "cmd = f\"python run_analysis.py --checkpoint output_dino/checkpoint.pth --images_dir {images_dir} --threshold {THRESHOLD}\"\n",
        "\n",
        "if annotations:\n",
        "    cmd += f\" --annotations {annotations} --format {FORMAT}\"\n",
        "\n",
        "if USE_OCR:\n",
        "    cmd += \" --use_ocr\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"–ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê\")\n",
        "print(\"=\"*60)\n",
        "print(f\"–ö–æ–º–∞–Ω–¥–∞: {cmd}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫–∞–µ–º\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "results_dir = \"results\"\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # –¢–∞–±–ª–∏—Ü–∞ 1: –°–≤–æ–¥–∫–∞ –ø–æ –≤–∏–¥–∞–º\n",
        "    summary_file = os.path.join(results_dir, \"summary_by_species.csv\")\n",
        "    if os.path.exists(summary_file):\n",
        "        print(\"\\nüìä –¢–ê–ë–õ–ò–¶–ê 1: –°–≤–æ–¥–∫–∞ –ø–æ –≤–∏–¥–∞–º –∂–∏–≤–æ—Ç–Ω—ã—Ö\\n\")\n",
        "        df_summary = pd.read_csv(summary_file)\n",
        "        display(df_summary)\n",
        "    \n",
        "    # –¢–∞–±–ª–∏—Ü–∞ 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥\n",
        "    detailed_file = os.path.join(results_dir, \"detailed_detections.csv\")\n",
        "    if os.path.exists(detailed_file):\n",
        "        print(\"\\nüìã –¢–ê–ë–õ–ò–¶–ê 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–π (–ø–µ—Ä–≤—ã–µ 30 –∑–∞–ø–∏—Å–µ–π)\\n\")\n",
        "        df_detailed = pd.read_csv(detailed_file)\n",
        "        display(df_detailed.head(30))\n",
        "        print(f\"\\n... –≤—Å–µ–≥–æ {len(df_detailed)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    \n",
        "    # ROC AUC –º–µ—Ç—Ä–∏–∫–∏\n",
        "    json_file = os.path.join(results_dir, \"analysis_results.json\")\n",
        "    if os.path.exists(json_file):\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            results = json.load(f)\n",
        "        \n",
        "        if 'roc_auc' in results and results['roc_auc']:\n",
        "            print(\"\\nüìà ROC AUC –ú–ï–¢–†–ò–ö–ò\\n\")\n",
        "            roc_data = results['roc_auc']\n",
        "            \n",
        "            if 'mean_roc_auc' in roc_data:\n",
        "                print(f\"  –°—Ä–µ–¥–Ω–∏–π ROC AUC: {roc_data['mean_roc_auc']:.4f}\")\n",
        "            \n",
        "            if 'per_class' in roc_data:\n",
        "                print(\"\\n  –ü–æ –∫–ª–∞—Å—Å–∞–º:\")\n",
        "                for class_name, auc_val in sorted(roc_data['per_class'].items(), key=lambda x: x[1], reverse=True):\n",
        "                    print(f\"    {class_name}: {auc_val:.4f}\")\n",
        "else:\n",
        "    print(\"[ERROR] –ü–∞–ø–∫–∞ results –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "if os.path.exists('results'):\n",
        "    shutil.make_archive('analysis_results', 'zip', 'results')\n",
        "    files.download('analysis_results.zip')\n",
        "    print(\"[OK] –ê—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è\")\n",
        "else:\n",
        "    print(\"[ERROR] –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù –ö–ª–∞—Å—Å—ã –∂–∏–≤–æ—Ç–Ω—ã—Ö\n",
        "\n",
        "–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤–∏–¥—ã:\n",
        "\n",
        "| ID | –†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ | English |\n",
        "|:--:|:-----------------|:--------|\n",
        "| 0 | –†–æ—Å–æ–º–∞—Ö–∞ | Wolverine |\n",
        "| 1 | –í—ã–¥—Ä–∞ | Otter |\n",
        "| 2 | –†—ã—Å—å | Lynx |\n",
        "| 3 | –ì–æ—Ä–Ω–æ—Å—Ç–∞–π | Ermine |\n",
        "| 4 | –í–æ–ª–∫ | Wolf |\n",
        "| 5 | –ú–µ–¥–≤–µ–¥—å | Bear |\n",
        "| 6 | –õ–æ—Å—å | Moose |\n",
        "| 7 | –û–ª–µ–Ω—å | Deer |\n",
        "| 8 | –õ–∏—Å–∏—Ü–∞ | Fox |\n",
        "| 9 | –ö–∞–±–∞–Ω | Wild boar |\n",
        "| 10 | –ó–∞—è—Ü-–±–µ–ª—è–∫ | Mountain hare |\n",
        "| 11 | –ë–µ–ª–∫–∞ | Squirrel |\n",
        "| 12 | –ì–ª—É—Ö–∞—Ä—å | Capercaillie |\n",
        "| 13 | –ö—É–Ω–∏—Ü–∞ | Marten |\n",
        "| 14 | –ï–Ω–æ—Ç–æ–≤–∏–¥–Ω–∞—è —Å–æ–±–∞–∫–∞ | Raccoon dog |\n",
        "| 15 | –ù–æ—Ä–∫–∞ | Mink |"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
